{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD9gUQpaBxNa"
      },
      "source": [
        "# How to Train YOLOv7 on a Custom Dataset\n",
        "\n",
        "This tutorial is based on the [YOLOv7 repository](https://github.com/WongKinYiu/yolov7) by WongKinYiu. This notebook shows training on **your own custom objects**. Many thanks to WongKinYiu and AlexeyAB for putting this repository together.\n",
        "\n",
        "\n",
        "### **Accompanying Blog Post**\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on [how to train YOLOv7](https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial/), concurrently.\n",
        "\n",
        "### **Steps Covered in this Tutorial**\n",
        "\n",
        "To train our detector we take the following steps:\n",
        "\n",
        "* Install YOLOv7 dependencies\n",
        "* Load custom dataset from Roboflow in YOLOv7 format\n",
        "* Run YOLOv7 training\n",
        "* Evaluate YOLOv7 performance\n",
        "* Run YOLOv7 inference on test images\n",
        "* OPTIONAL: Deployment\n",
        "* OPTIONAL: Active Learning\n",
        "\n",
        "\n",
        "## Roboflow Universe\n",
        "\n",
        "Need data for your project? Before spending time on annotating, check out Roboflow Universe, a repository of more than 200,000 open-source datasets that you can use in your projects. You'll find datasets containing everything from annotated cracks in concrete to plant images with disease annotations.\n",
        "\n",
        "\n",
        "[![Roboflow Universe](https://media.roboflow.com/notebooks/template/uni-banner-frame.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672878480290)](https://universe.roboflow.com/)\n",
        "\n",
        "## Preparing a customÂ dataset\n",
        "\n",
        "Building a custom dataset can be a painful process. It might take dozens or even hundreds of hours to collect images, label them, and export them in the proper format. Fortunately, Roboflow makes this process as straightforward and fast as possible. Let me show you how!\n",
        "\n",
        "### Step 1: Creating project\n",
        "\n",
        "Before you start, you need to create a Roboflow [account](https://app.roboflow.com/login). Once you do that, you can create a new project in the Roboflow [dashboard](https://app.roboflow.com/). Keep in mind to choose the right project type. In our case, Object Detection.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/creating-project.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929799852\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 2: Uploading images\n",
        "\n",
        "Next, add the data to your newly created project. You can do it via API or through our [web interface](https://docs.roboflow.com/adding-data/object-detection).\n",
        "\n",
        "If you drag and drop a directory with a dataset in a supported format, the Roboflow dashboard will automatically read the images and annotations together.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/uploading-images.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929808290\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 3: Labeling\n",
        "\n",
        "If you only have images, you can label them in [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://user-images.githubusercontent.com/26109316/210901980-04861efd-dfc0-4a01-9373-13a36b5e1df4.gif\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 4: Generate new dataset version\n",
        "\n",
        "Now that we have our images and annotations added, we can Generate a Dataset Version. When Generating a Version, you may elect to add preprocessing and augmentations. This step is completely optional, however, it can allow you to significantly improve the robustness of your model.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/generate-new-version.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1673003597834\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 5: Exporting dataset\n",
        "\n",
        "Once the dataset version is generated, we have a hosted dataset we can load directly into our notebook for easy training. Click `Export` and select the `YOLO v5 PyTorch` dataset format.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/export.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672943313709\"\n",
        "  >\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD-uPyQ_2jiN",
        "outputId": "23ccb846-0bd6-4141-b8b6-d05bb70ebf5d"
      },
      "outputs": [],
      "source": [
        "# Create a virtual environment first!\n",
        "\n",
        "# run these in your terminal:\n",
        "# python -m venv env\n",
        "# ./env/Scripts/activate.bat\n",
        "\n",
        "# if the above doesn't work, try this one:\n",
        "# ./env/Scripts/Activate.ps!\n",
        "import sys\n",
        "assert sys.prefix != sys.base_prefix, \"Please activate your virual environment. See the comment in the code cell.\"\n",
        "\n",
        "# Download YOLOv7 repository and install requirements\n",
        "# A custom version of YoloV7 that works with pytorch>1.12.1 and numpy>1.20.1,\n",
        "# and supports SnapML\n",
        "!git clone https://github.com/gyfen/yolov7-snapml.git\n",
        "\n",
        "%pip install -r requirements.txt\n",
        "# %pip install albumentations\n",
        "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
        "# %pip install -r yolov7-snapml/requirements.txt\n",
        "# %pip install protobuf\n",
        "# %pip install onnx>=1.9.0\n",
        "# %pip install onnx-simplifier>=0.3.6\n",
        "# %pip install roboflow\n",
        "# %pip install pyyaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtJ24mPlyF-S"
      },
      "source": [
        "# Download Correctly Formatted Custom Data\n",
        "\n",
        "Next, we'll download our dataset in the right format. Use the `YOLOv7 PyTorch` export. Note that this model requires YOLO TXT annotations, a custom YAML file, and organized directories. The roboflow export writes this for us and saves it in the correct spot.\n",
        "\n",
        "You can find our project here: https://app.roboflow.com/diminishedreality/food-products-jnugg/17\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovKgrVN8ygdW",
        "outputId": "08c49885-6ef5-42f7-af24-a5f5531457c2"
      },
      "outputs": [],
      "source": [
        "# REPLACE with your custom code snippet generated above\n",
        "\n",
        "API_KEY = \"YOUR_KEY_HERE\"\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=API_KEY)\n",
        "project = rf.workspace(\"diminishedreality\").project(\"food-products-jnugg\")\n",
        "dataset = project.version(17).download(\"yolov7\")\n",
        "\n",
        "print(\"Dataset downloaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hod-8RoSSyE0"
      },
      "source": [
        "# Augment images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28awT5PNB4ys",
        "outputId": "e1883203-dee6-4a14-f5a2-4355f546b2b2"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "\n",
        "# amt of augmentations per image\n",
        "N = 1\n",
        "\n",
        "# locations\n",
        "train_loc = Path(dataset.location) / \"train\"\n",
        "images_loc = train_loc / \"images\"\n",
        "labels_loc = train_loc / \"labels\"\n",
        "\n",
        "# transformations to apply\n",
        "transform = A.Compose(\n",
        "    [\n",
        "        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
        "        A.RandomScale(scale_limit=(-0.3, 0.2), p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.MotionBlur(blur_limit=3, p=0.2),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(\n",
        "        format=\"yolo\",\n",
        "        label_fields=[\"category_ids\"],\n",
        "        min_visibility=0.7,\n",
        "        clip=True,\n",
        "        filter_invalid_bboxes=True,\n",
        "    ),\n",
        ")\n",
        "\n",
        "img_paths = list(images_loc.glob(\"*.jpg\"))\n",
        "\n",
        "for img_path in img_paths:\n",
        "    stem = img_path.stem\n",
        "\n",
        "    label_path = labels_loc / f\"{stem}.txt\"\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    height, width = img.shape[:2]\n",
        "\n",
        "    bboxes = []\n",
        "    class_ids = []\n",
        "\n",
        "    with open(label_path, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            cls, x, y, w, h = map(float, line.strip().split())\n",
        "            bboxes.append([x, y, w, h])\n",
        "            class_ids.append(int(cls))\n",
        "\n",
        "    for i in range(N):\n",
        "        # Apply augmentation\n",
        "        augmented = transform(image=img, bboxes=bboxes, category_ids=class_ids)\n",
        "        aug_img = augmented[\"image\"]\n",
        "        aug_bboxes = augmented[\"bboxes\"]\n",
        "        # aug_classes = augmented[\"category_ids\"]\n",
        "\n",
        "        aug_img_name = f\"{stem}_aug{i}.jpg\"\n",
        "        aug_label_name = f\"{stem}_aug{i}.txt\"\n",
        "\n",
        "        # Save new image\n",
        "        cv2.imwrite(images_loc / aug_img_name, aug_img)\n",
        "\n",
        "        # Save new label file\n",
        "        with open(labels_loc / aug_label_name, \"w\") as f:\n",
        "            for cls_id, (x, y, w, h) in zip(class_ids, aug_bboxes):\n",
        "                f.write(f\"{cls_id} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n",
        "\n",
        "print(\"Augmentation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHfT9gEiBsBd"
      },
      "source": [
        "# Begin Custom Training\n",
        "\n",
        "We're ready to start custom training.\n",
        "If you'd like to change any settings, see details in [our accompanying blog post](https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iqOPKjr22mL",
        "outputId": "0c974fda-504a-4fe0-9535-f6249157a489"
      },
      "outputs": [],
      "source": [
        "# run this cell to begin training\n",
        "\n",
        "IMG_SIZE = 640\n",
        "EPOCHS = 1\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "!python yolov7-snapml/train.py --workers 8 --device 0 --batch 16 --epochs {EPOCHS} --img {IMG_SIZE} {IMG_SIZE} --data \"{dataset.location}/data.yaml\" --weights yolov7-snapml/yolov7-tiny.pt --cfg yolov7-snapml/cfg/training/yolov7-tiny.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W0MpUaTCJro"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "We can evaluate the performance of our custom training using the provided evalution script.\n",
        "\n",
        "Note we can adjust the below custom arguments. For details, see [the arguments accepted by detect.py](https://github.com/WongKinYiu/yolov7/blob/main/detect.py#L154)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4cfnLtTCIce",
        "outputId": "6c4f36d8-bc8a-4656-8cb1-837604ebe331"
      },
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "import yaml\n",
        "with open(dataset.location + \"\\\\data.yaml\", 'r') as file:\n",
        "    y = yaml.safe_load(file)\n",
        "\n",
        "y['test'] = f'{dataset.location.split(\"\\\\\")[-1]}/test/images'\n",
        "\n",
        "with open(dataset.location + \"\\\\data.yaml\", 'w') as f:\n",
        "    yaml.dump(y, f)\n",
        "\n",
        "!python yolov7-snapml/test.py --weights runs/exp/weights/best.pt --data \"{dataset.location}/data.yaml\" --task test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwH3PwXjfkXq"
      },
      "source": [
        "# Export the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLBQF5Wefj4x",
        "outputId": "74032b5a-6b4d-427b-876c-cb5d3f081928"
      },
      "outputs": [],
      "source": [
        "!python export.py --weights runs/exp/weights/best.pt --grid --simplify --export-snapml --img-size {IMG_SIZE} {IMG_SIZE} --max-wh {IMG_SIZE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVpCFeU-K4gb"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Congratulations, you've trained a custom YOLOv7 model!\n",
        "Now take the onnx file and bring it into LensStudio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
